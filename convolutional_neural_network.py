# -*- coding: utf-8 -*-
"""Convolutional neural network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uahy4wiRzZYzGYhNAHXF7GB1_hAymheL
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
import torch.nn.functional as F

# scratch
class CNN_model(nn.Module):
  def __init__(self):
    super(net,self).__init__()
    self.conv1=nn.Conv2d(1,32,3,1)  #input,filters,kernel_size,stride
    self.conv2=nn.Conv2d(32,64,3,1)
    self.fc1=nn.Linear(1233,120)    # input_feature,output 120
    self.fc2=nn.Linear(120,10)

  def forword(self,x):
    x=self.conv1(x)
    x=F.relu(x)
    x=self.conv2(x)
    x=F.relu(x)
    x=F.max_pool2d(x,2)
    x=torch.flatten(x,1) # resize to one diminsion
    x=self.fc1(x)
    x=F.relu(x)
    x=self.fc2(x)
    output=F.log_softmax(x,dim=1)
    return output

model=net()

#data
train_data=DataLoader(tain_data,batch_sampler=4,shuffle=4,transforms=ToTensor())
test_data=DataLoader(tst_data,batch_size=5shuffle=4,transforms=ToTensor())

# loss and optimiser
optimiser=optim.Adam(model.parameters(), lr=0.001)
criterion=nn.MSELoss()

#training loop
num_epoch=10
for epoch in range(num_epoch):
  for input,labels in train_data:

    #zero grad
    optimiser.zero_grad()

    #forward
    output=model(input)
    _,predict=output.max(1)
    loss=criterion(output,labels)

    #backpropogation
    loss.backward()
    optimiser.step()

print('training_loop completed')


# model evaluation
model.eval()
with torch.no_grad():
  output=model(input)
  _,predict=output.max(1)
  print(predict)

# prepossing
prepross=transforms.Compose([
    transforms.RandomHorizontalFlip()
    transforms.RandomResizedCrop(120)
    transforms.ToTensor()
    transforms.F.normalize([1,1,1],[1,1,1])
])

image=prepross(image)

# datasets
train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())
test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())

# load the data
from torch.utils.data import DataLoader
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(test_data, batch_size=64, shuffle=False)

# loss and optimiser
crition=nn.CrossEntropyLoss()
optimiser=optim.SGD(model.parameters(),lr=0.01)

# Training loop (pretrianed model)
model=models.resnet18(pretrained=True)
num_epochs = 1
for epoch in range(num_epochs):
    for input,labels in train_data:

      optimiser.zero_grad()
      #forward pass
      output=model(input)
      _,predict=output.max(1)
      loss=crition(output,labels)
      # backward pass
      loss.backward()
      optimiser.step()

      print(f'print{loss}')

torch.save(model.state_dict(),'model.pth')

model.eval()
with torch.no_grad():
  for input,labels in test_data:
    output=model(input)
    _,predict=output.max(1)
    loss=crition(output,labels)

# To alter the data into required form
prepross=transforms.Compose([
    transforms.RandomResizedCrop(120)
    transforms.RandomHorizontalFlip()
    transforms.ToTensor()
    transforms.Normalize([1,2,2],[1,1,1])
])

image=prepross(image)

# Alter the entire data into required form
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val':transforms.Compose([
        transforms.RandomResizedCrop(120)
    ]) }

"""To understand the setps and function and uses"""

import torch

# Example tensor with shape [32, 1, 5, 5]
t = torch.randn(32, 1, 5, 5)
# Flatten the tensor starting from the second dimension
flattened_t = torch.flatten(t, 1)
print(flattened_t.shape) # Output: torch.Size([32, 25])

# To check the model accuracy with for generative model GAN
from pytorch_gan_matrics import InceptionScore, FrechetInceptionDistance

with torch.no_grad():
  fake_image=generator(torch.randn(real_images.size(0), 100))
# It uses a pre-trained Inceptionv3 model to calculate the expected log-likelihood of the generated images. A higher IS indicates that the generated images are more likely to be realistic and diverse.
InceptionScore=InceptionScore(fake_image,real_image)

# A lower FID indicates that the generated images are closer to the real images in the feature space, suggesting better performance of the GAN.
FrechetInceptionDistance=FrechetInceptionDistance(fake_image,real_image)

"""**Keras framework**"""

from keras.models import Sequencial
from keras.layers import Dense
from keras.optimiser import Adam

def genrator():
  model.Sequencial()
  model.add(Dense(unit=100,input_dim=100,activation='relu'))
  model.add(Dense(unit=50,activation='relu'))
  return model

# # In PyTorch, there are two ways to flatten a tensor. The first way is to use the torch.flatten() method. This method takes a tensor as input and returns a one-dimensional tensor. The second way is to use the view() method. This method takes a tensor and a new shape as input and returns a tensor with the new shape.
# # Here is an example of how to use the torch.flatten() method to flatten a tensor:
# # Python

# import torch

# # Create a tensor
# x = torch.tensor([[1, 2, 3], [4, 5, 6]])
# # Flatten the tensor
# y = torch.flatten(x)
# # Print the flattened tensor
# print(y)
# # Output:
# # Code

# # tensor([1, 2, 3, 4, 5, 6])
# # Here is an example of how to use the view() method to flatten a tensor:
# # Python
# import torch

# # Create a tensor
# x = torch.tensor([[1, 2, 3], [4, 5, 6]])

# # Flatten the tensor
# y = x.view(-1)

# # Print the flattened tensor
# print(y)
# Output:
# # Code

# # tensor([1, 2, 3, 4, 5, 6])
# # As you can see, both methods produce the same result. The torch.flatten() method is more concise, but the view() method is more flexible. You can use the view() method to reshape a tensor into any shape, not just a one-dimensional shape.
# # Here is an example of how to use the view() method to reshape a tensor into a two-dimensional shape:
# # Python

# import torch

# # Create a tensor
# x = torch.tensor([[1, 2, 3], [4, 5, 6]])
# # Reshape the tensor into a two-dimensional shape
# y = x.view(2, 3)
# # Print the reshaped tensor
# print(y)
# # Output:
# # Code
# # tensor([[1, 2, 3],
# #         [4, 5, 6]])
# # The view() method is a powerful tool that can be used to reshape tensors into any shape you need.

import numpy as np
x=np.random.random(2)
print(x)
y=np.random.randn(2,3)
print(y)
z=np.random.randint(2,5,(2,3))
print(f'random int {z} diminsion {z.ndim}')